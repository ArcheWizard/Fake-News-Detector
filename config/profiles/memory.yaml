# Memory-friendly preset (low VRAM / CPU)
# Trades speed for lower memory consumption
max_seq_length: 192
train:
  epochs: 2
  batch_size: 8
  learning_rate: 2.0e-5
  warmup_ratio: 0.05
  evaluation_strategy: "epoch"
  save_strategy: "epoch"
  logging_steps: 100
  gradient_accumulation_steps: 2
  dataloader_num_workers: 0
  lr_scheduler_type: "linear"
  gradient_checkpointing: true
  fp16: false
  bf16: false
  optim: "adamw_torch"
  torch_compile: false
